seek_VB to_TO find_VB a_DT weak_JJ i_LS -RRB-_-RRB- learner_NN which_WDT minimizes_VBZ weighted_JJ error_NN rate_NN ,_, with_IN the_DT weights_NNS being_VBG the_DT gradient_NN of_IN the_DT loss_NN ._.
If_IN we_PRP use_VBP the_DT exponential_JJ loss_NN :_: C_NN -LRB-_-LRB- y_NN ,_, f_LS -RRB-_-RRB- =_JJ exp_NN -LRB-_-LRB- âˆ’_CD yf_NN -RRB-_-RRB- -LRB-_-LRB- 2_LS -RRB-_-RRB- then_RB it_PRP can_MD be_VB shown_VBN -LRB-_-LRB- e.g._FW =_JJ -_: =[_NN 13_CD -RRB-_-RRB- -_: =--RRB-_NN that_WDT -LRB-_-LRB- 1_LS -RRB-_-RRB- is_VBZ the_DT exact_JJ classification_NN task_NN which_WDT AdaBoost_NNP -LRB-_-LRB- 9_CD -RRB-_-RRB- ,_, the_DT original_JJ and_CC most_RBS famous_JJ boosting_VBG algorithm_NN ,_, solves_VBZ for_IN finding_VBG the_DT next_JJ weak_JJ learner_NN ._.
In_IN their_PRP$ original_JJ AdaBoost_NNP implementation_NN -LRB-_-LRB- 8_CD -RRB-_-RRB-
