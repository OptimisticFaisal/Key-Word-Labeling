of_IN at_IN most_JJS 1\/2_CD −_FW λ_FW for_IN each_DT iteration_NN --_: then_RB the_DT training_NN margins_NNS will_MD increase_VB ,_, and_CC generalization_NN error_NN will_MD go_VB to_TO 0_CD for_IN large_JJ enough_JJ training_NN samples_NNS -LRB-_-LRB- see_VB -LRB-_-LRB- 20_CD -RRB-_-RRB- for_IN more_JJR details_NNS -RRB-_-RRB- ._.
Duffy_NN and_CC Helmbold_JJ -LRB-_-LRB- =_JJ -_: =[_NN 7_CD -RRB-_-RRB- -_: =_JJ -_: ,_, Theorems_NNP 24_CD -RRB-_-RRB- give_VBP sufficient_JJ conditions_NNS for_IN this_DT property_NN to_TO hold_VB ,_, which_WDT apply_VBP to_TO the_DT exponential_JJ and_CC logistic_JJ loss_NN functions_NNS ._.
These_DT conditions_NNS include_VBP strict_JJ convexity_NN ,_, and_CC so_RB do_VBP not_RB directly_RB apply_VB
