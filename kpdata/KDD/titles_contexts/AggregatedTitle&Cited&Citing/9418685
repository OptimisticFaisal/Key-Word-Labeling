Training_VBG structural_JJ svms_NNS with_IN kernels_NNS using_VBG sampled_VBN cuts_NNS
ls_NNS is_VBZ a_DT well-studied_JJ problem_NN ,_, and_CC there_EX are_VBP stable_JJ SVM_NN solvers_NNS that_WDT are_VBP suitable_JJ for_IN comparisons_NNS ._.
Moreover_RB ,_, scaling_VBG up_RP SVM_NN with_IN kernels_NNS to_TO large_JJ datasets_NNS is_VBZ an_DT interesting_JJ research_NN problem_NN on_IN its_PRP$ own_JJ =_JJ -_: =[_NN 1_CD -RRB-_-RRB- -_: =_SYM -_: ._.
In_IN binary_JJ classification_NN the_DT loss_NN function_NN ∆_NN is_VBZ just_RB the_DT zeroone_NN loss_NN ._.
The_DT feature_NN map_NN Ψ_NN is_VBZ defined_VBN by_IN Ψ_NN -LRB-_-LRB- x_NN ,_, y_NN -RRB-_-RRB- =_JJ yφ_NN -LRB-_-LRB- x_NN -RRB-_-RRB- ,_, where_WRB y_FW ∈_FW -LCB-_-LRB- 1_CD ,_, −_NN 1_CD -RCB-_-RRB- and_CC φ_NN is_VBZ the_DT nonlinear_JJ feature_NN map_NN induced_VBN from_IN a_DT Mercer_NNP kerne_NN
he_PRP Nyström_NNP method_NN has_VBZ been_VBN proposed_VBN in_IN -LRB-_-LRB- 18_CD -RRB-_-RRB- to_TO approximate_JJ the_DT kernel_NN matrix_NN used_VBN for_IN Gaussian_NNP Process_VB classification_NN ._.
Low-rank_JJ approximation_NN has_VBZ been_VBN exploited_VBN to_TO speed_VB up_RP the_DT training_NN of_IN kernel_NN SVMs_NNS =_JJ -_: =[_NN 2_CD -RRB-_-RRB- -_: =_SYM -_: ._.
A_DT greedy_JJ basis-pursuit-style_JJ algorithm_NN is_VBZ also_RB proposed_VBN in_IN -LRB-_-LRB- 7_CD -RRB-_-RRB- to_TO build_VB sparse_JJ kernel_NN SVMs_NNS to_TO speed_VB up_RP both_CC training_NN and_CC classification_NN ._.
4_LS ._.
THE_DT CUT_NNP SUBSAMPLING_NNP ALGORITHMS_NNPS Our_PRP$ main_JJ idea_NN is_VBZ to_TO speed_VB u_NN
g_NN and_CC learning_VBG ranking_JJ functions_NNS ._.
Second_RB ,_, they_PRP allow_VBP optimizing_VBG directly_RB to_TO non-standard_JJ loss_NN functions_NNS that_WDT do_VBP not_RB necessarily_RB have_VB to_TO decompose_VB linearly_RB -LRB-_-LRB- e.g._FW Average_NNP Precision_NNP ,_, ROC-Area_NN ,_, F1-score_NN -RRB-_-RRB- =_JJ -_: =[_NN 4_CD ,_, 20_CD -RRB-_-RRB- -_: =_SYM -_: ._.
And_CC third_JJ ,_, their_PRP$ runtime_NN provably_RB scales_NNS linearly_RB with_IN the_DT number_NN of_IN training_NN examples_NNS for_IN linear_JJ models_NNS ._.
This_DT makes_VBZ cutting-plane_JJ methods_NNS not_RB only_RB attractive_JJ for_IN training_NN structured_JJ prediction_NN mode_NN
This_DT makes_VBZ cutting-plane_JJ methods_NNS not_RB only_RB attractive_JJ for_IN training_NN structured_JJ prediction_NN models_NNS ,_, but_CC they_PRP are_VBP also_RB orders_NNS of_IN magnitude_NN faster_RBR than_IN conventional_JJ methods_NNS for_IN training_NN binary_JJ classifiers_NNS =_JJ -_: =[_NN 5_CD -RRB-_-RRB- -_: =_SYM -_: ._.
Unfortunately_RB ,_, the_DT computational_JJ efficiency_NN of_IN cutting-plane_JJ methods_NNS becomes_VBZ substantially_RB worse_JJR for_IN non-linear_JJ models_NNS that_WDT involve_VBP kernels_NNS ._.
While_IN it_PRP is_VBZ possible_JJ to_TO train_VB kernel_NN models_NNS ,_, the_DT computat_NN
the_DT popular_JJ shrinking_NN heuristic_NN used_VBN in_IN training_NN SVMs_NNS ._.
On_IN the_DT other_JJ hand_NN ,_, one_CD major_JJ bottleneck_NN in_IN the_DT speed_NN of_IN the_DT current_JJ algorithm_NN is_VBZ the_DT large_JJ number_NN of_IN cuts_NNS required_VBN before_IN convergence_NN ._.
Recently_RB =_JJ -_: =[_NN 3_CD -RRB-_-RRB- -_: =_SYM -_: proposes_VBZ a_DT stabilized_VBN cutting_NN plane_NN algorithm_NN for_IN linear_JJ SVMs_NNS with_IN much_JJ improved_JJ convergence_NN ,_, and_CC it_PRP will_MD be_VB interesting_JJ to_TO extend_VB their_PRP$ techniques_NNS to_TO improve_VB the_DT speed_NN of_IN our_PRP$ sampled-cut_JJ algorithm_NN f_SYM
